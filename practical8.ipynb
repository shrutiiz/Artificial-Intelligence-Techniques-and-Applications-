{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## This code demonstrates a simple NLP application, including data preprocessing, tokenization, stemming, lemmatization, and text classification using the Bag of Words and TF-IDF methods. This example uses the scikit-learn, nltk, and pandas libraries.\n",
        "\n",
        "The application is based on sentiment analysis, where we classify movie reviews as positive or negative."
      ],
      "metadata": {
        "id": "rDnJKEub9iFN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TzfzUJ59cwR",
        "outputId": "101e698b-301a-42e7-d441-85801df9956c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk scikit-learn pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "# Include 'punkt_tab' in the downloads\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True) # Download the Punkt Sentence Tokenizer\n",
        "\n",
        "# Example dataset: Movie reviews (positive/negative)\n",
        "data = {\n",
        "    'text': [\"I love this movie\", \"This was a terrible movie\", \"Best movie ever!\", \"Worst movie I have ever seen\",\n",
        "             \"I enjoyed this movie\", \"It was a horrible movie\", \"Amazing movie\", \"I hated this movie\",\n",
        "             \"This movie was fantastic\", \"Terrible movie\"],\n",
        "    'label': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
        "}\n",
        "\n",
        "# Load data into DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Preprocessing: Lowercasing and removing stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text, use_stemming=True, use_lemmatization=False):\n",
        "    # Tokenize text\n",
        "    words = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
        "\n",
        "    if use_stemming:\n",
        "        # Apply stemming\n",
        "        words = [stemmer.stem(word) for word in words]\n",
        "    elif use_lemmatization:\n",
        "        # Apply lemmatization\n",
        "        words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply text preprocessing (choose stemming or lemmatization)\n",
        "df['processed_text'] = df['text'].apply(lambda x: preprocess_text(x, use_stemming=True))\n",
        "\n",
        "# Vectorization using TF-IDF (you can also use CountVectorizer if preferred)\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['processed_text'])\n",
        "\n",
        "# Target variable\n",
        "y = df['label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Check the shape of the splits to ensure it isn't too small for training\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n",
        "\n",
        "# Model: Naive Bayes classifier\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation: Accuracy Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Test prediction on new text\n",
        "new_text = \"I absolutely loved this film!\"\n",
        "processed_new_text = preprocess_text(new_text, use_stemming=True)\n",
        "X_new = vectorizer.transform([processed_new_text])\n",
        "prediction = model.predict(X_new)\n",
        "print(f\"Sentiment of the new review: {'Positive' if prediction[0] == 1 else 'Negative'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYt2ldLR6KaA",
        "outputId": "99d213dc-9ee4-4bc9-ae78-017e7a63c1e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (7, 12)\n",
            "Test set size: (3, 12)\n",
            "Accuracy: 66.67%\n",
            "Sentiment of the new review: Positive\n"
          ]
        }
      ]
    }
  ]
}